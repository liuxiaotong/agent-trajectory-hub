# Agent Trajectory Hub — 项目路线图

## 一、项目定位

**一句话定义：** 面向 Code Agent 的 RL 环境，产出带过程级 Reward 的执行轨迹数据，用于 SFT/DPO 训练。

**核心论点：**

- SWE-bench Pro（Scale AI）证明了现有评测只看结果，不看过程
- CL-bench（腾讯）揭示了模型不会从上下文中学习，倾向于凭记忆瞎猜
- 我们的项目：提供过程级数据，让 Agent 学会"怎么做"，而不只是"做没做出来"

**为什么从代码场景开始：**

- 工具集封闭且可枚举（file_read, file_write, shell, search, git），约 5-10 个
- 环境可控：一个 Docker 容器 = 整个"世界"
- 结果可验证：测试通过/失败，确定性的
- 扩展到全领域需要"复制一个世界"，代码场景是最可行的起点

---

## 二、架构总览

### 编排层 + 原子能力

```
                  agent-trajectory-hub（编排层）
                            │
       ┌────────────────────┼────────────────────┐
       │                    │                    │
  任务层 (Task)        执行层 (Exec)        价值层 (Value)
       │                    │                    │
  ┌────┴────┐       ┌──────┼──────┐      ┌──────┼──────┐
  │         │       │      │      │      │      │      │
任务源   Recipe  Sandbox Recorder Reward  训练    评测   发布
(外部)  (复用)   (新建)  (新建)   (新建)  实验
                                   │
                          ┌────────┼────────┐
                          │        │        │
                       Check    Synth    Label
                      (复用)    (复用)    (复用)
```

### 现有项目复用

| 现有项目 | 在新项目中的角色 |
|---|---|
| **Radar** | 持续监控竞品数据集和评测基准动态 |
| **Recipe** | 定义轨迹数据的 Schema，生成配套文档 |
| **Synth** | Reward 的模型打分层（LLM-as-Judge） |
| **Label** | 偏好对的人工标注 + IAA 一致性验证 |
| **Check** | Reward 的规则层 + 轨迹数据质检 |

### 需要新建的原子能力

| 新模块 | 职责 | 说明 |
|---|---|---|
| **agent-sandbox** | 可复现的代码执行环境 | Docker 沙箱，暴露标准工具接口，支持重置和重放 |
| **agent-recorder** | 标准化轨迹录制 | 拦截 Agent ↔ Sandbox 交互，输出统一 Schema |
| **agent-reward** | 过程级 Reward 计算 | 规则层（Check）+ 模型层（Synth）+ 人工校准（Label） |

---

## 三、Reward 设计（核心差异化）

借鉴 CL-bench 的 rubric 方法论，每一步用多维评分标准打分，而非单一分数：

```
每步 Reward = 多维 Rubric 加权求和

rubric_1: 目标推进 — "这一步是否推进了任务目标？"
rubric_2: 工具选择 — "选择的工具是否合理？"
rubric_3: 参数正确 — "工具调用的参数是否正确？"
rubric_4: 信息利用 — "是否利用了之前获得的信息？"
rubric_5: 非冗余   — "这一步是否是冗余操作？"

step_reward = w1*r1 + w2*r2 + w3*r3 + w4*r4 + w5*r5
```

Reward 计算的三层架构：

| 层 | 实现方式 | 复用项目 | 特点 |
|---|---|---|---|
| 规则层 | 确定性规则（重复操作检测、步数效率、测试通过率） | Check | 客观、可审计、零成本 |
| 模型层 | LLM-as-Judge 逐步评判 + 理由 | Synth | 灵活、可解释、有成本 |
| 人工校准层 | 人工对关键 case 做偏好标注，计算 reward 与人类判断的相关性 | Label | 权威、用于验证前两层的可靠性 |

---

## 四、信任链（客户如何相信数据好）

端到端可审计，每一步都有证据：

```
任务可追溯 → 执行可重放 → Reward 可解释 → 偏好可校验 → 效果可复现
```

| 环节 | 证据 | 怎么验证 |
|---|---|---|
| 任务来源 | 来自真实开源项目的单元测试 | 客户可去 GitHub 核实 |
| 执行过程 | 每条轨迹可在沙箱中重放 | 客户拿轨迹重放，结果一致 |
| Reward 打分 | 规则层触发了哪条规则 + 模型层的打分理由 | 客户可逐条审阅 |
| 偏好对 | 多标注员标注 + IAA 一致性分数 | 客户可看一致性报告 |
| 训练效果 | 用数据训练后在公认基准上的分数提升 | 训练脚本开源，客户可自己跑 |

**最关键的证据：训练实验。** 必须证明"用了数据 → 模型在 SWE-bench Pro / Verified 上分数提升"。

---

## 五、分阶段路线图

### Phase 0：立项与基础设施（第 1-2 周）

**目标：** 搭好骨架，定义清楚"做什么"。

| 事项 | 产出 | 负责模块 |
|---|---|---|
| 创建 agent-trajectory-hub 主仓库 | GitHub repo + 项目结构 | — |
| 设计轨迹数据 Schema v0.1 | `schema/trajectory.json` | Recipe |
| 设计 Rubric-based Reward Schema v0.1 | `schema/reward.json` | — |
| 竞品差距分析 | 对比文档（vs SWE-bench Pro / CL-bench / SWE-Flow） | Radar |
| 确定第一批任务来源 | 从 SWE-bench Pro 公开集选 100 个任务 | — |

**Phase 0 检查点：** Schema 定义完成，任务选好，可以开始跑了。

---

### Phase 1：最小闭环验证（第 3-8 周）

**目标：** 证明一件事——用这条 pipeline 产出的数据训练模型，效果有正向提升。

这是整个项目的 GO / NO-GO 关卡。

#### 第 3-4 周：采集轨迹

| 事项 | 产出 | 说明 |
|---|---|---|
| 搭建执行环境 | 可运行的 Agent 环境 | Phase 1 直接用 OpenHands（不自建沙箱） |
| 选定 2-3 个模型 | 模型列表 | 如 Qwen2.5-Coder-32B / DeepSeek-V3 / Claude Sonnet |
| 每个模型跑 100 个任务 | 200-300 条原始轨迹 | OpenHands 框架执行 |
| 写日志解析器 | `recorder/openhands_adapter.py` | 把 OpenHands 日志转成标准 Schema |

#### 第 5-6 周：打分 + 构建偏好对

| 事项 | 产出 | 复用项目 |
|---|---|---|
| 实现规则层 Reward | 规则打分结果 | Check（扩展规则集） |
| 同一任务的多条轨迹按 reward 排序 | 初步偏好对 | — |
| 人工抽检 30 对偏好 | Reward 校准报告 | Label |
| 检验 reward 与人类判断一致性 | Pearson r 值 | — |

#### 第 7-8 周：训练 + 评测

| 事项 | 产出 | 说明 |
|---|---|---|
| 用轨迹数据做 SFT | 微调后的模型 | 基座：Qwen2.5-Coder-7B |
| 用偏好对做 DPO | DPO 后的模型 | — |
| 在 SWE-bench Verified 上评测 | 分数对比 | 对照组：原始模型 / 只用 SWE-Flow 数据 |
| 分析结果，写实验报告 | 实验报告 | **最关键的交付物** |

#### Phase 1 判断标准

| 结果 | 下一步 |
|---|---|
| 实验组明显优于对照组（+3% 以上） | 进 Phase 2，扩规模 |
| 略优或持平 | 分析原因：任务不够？reward 太粗？数据量不够？ |
| 不如对照组 | 停下来，重新审视 reward 设计和数据质量 |

---

### Phase 2：Reward 精细化 + 环境独立（第 9-16 周）

**前提：Phase 1 验证数据有正向效果。**

#### Reward 精细化（第 9-12 周）

| 事项 | 产出 | 复用项目 |
|---|---|---|
| 实现 Rubric-based 多维 Reward | 每步多维度打分 | — |
| 加入模型打分层（LLM-as-Judge） | 模型打分 + 理由 | Synth |
| 人工标注 200+ 偏好对 | 高质量偏好数据 | Label |
| Reward 校准实验 | reward vs 人类判断 Pearson r > 0.8 | Label + Check |
| 发布 Reward 校准报告 | 公开报告 | — |

#### 环境独立化（第 13-16 周）

| 事项 | 产出 | 说明 |
|---|---|---|
| 抽出 agent-sandbox 独立模块 | 独立 PyPI 包 | 从 OpenHands 依赖中解耦 |
| 抽出 agent-recorder 独立模块 | 独立 PyPI 包 | 通用适配器模式 |
| 支持更多 Agent 框架 | 适配器：SWE-agent / 自定义 Agent | — |
| 实现轨迹重放验证 | 重放工具 | 客户可审计 |
| 用扩大的数据集重新做训练实验 | 更大规模实验报告 | — |

---

### Phase 3：规模化 + 开源发布（第 17-24 周）

**前提：Phase 2 的 Reward 校准达标，训练效果持续正向。**

| 事项 | 产出 | 说明 |
|---|---|---|
| 任务池扩到 500+ | 多语言（Python/Go/JS/TS）、多类型 | — |
| 轨迹扩到 3000+ | 覆盖更多模型和 Agent 框架 | — |
| 偏好对扩到 1000+ | 高质量 DPO 数据 | Label |
| HuggingFace 发布 v1.0 数据集 | 数据集 + 完整数据卡片 | — |
| GitHub 发布全套工具 | sandbox + recorder + reward | — |
| arXiv 发布技术报告 | 论文 | — |
| 建立评测排行榜 | Agent 能力排名 | — |

---

### Phase 4：商业化 + 持续迭代（第 25 周起）

| 事项 | 说明 |
|---|---|
| 数据授权服务 | 客户购买带 reward 的轨迹数据 |
| 定制化数据服务 | 客户指定任务领域 → 产出对应数据 |
| 评测即服务 | 客户提交 Agent → 在你的环境里评测 → 出报告 |
| 持续更新 | Radar 监控 → 任务池迭代 → 数据集版本更新 |

---

## 六、里程碑总览

```
第 2 周  ── Schema 定义完成、任务选定           → Phase 0 完成
第 8 周  ── 训练实验出结果（GO / NO-GO 关卡）    → Phase 1 完成
第 12 周 ── Reward 校准 Pearson r > 0.8        → Reward 验证通过
第 16 周 ── 环境独立化、更大规模实验             → Phase 2 完成
第 24 周 ── v1.0 数据集 + 工具 + 论文发布        → Phase 3 完成
第 25 周+ ── 商业化运营                         → Phase 4 持续
```

---

## 七、关键风险与应对

| 风险 | 影响 | 应对 |
|---|---|---|
| Phase 1 训练实验无正向效果 | 项目方向存疑 | 先分析原因（数据量/reward/任务质量），小范围调整后重试 |
| Reward 与人类判断相关性低 | 数据价值不可信 | 增加规则层权重，减少模型层依赖；扩大人工校准样本 |
| OpenHands 日志格式变更 | 适配器失效 | Phase 2 建独立沙箱后不再依赖 |
| 竞品先发布类似项目 | 差异化减弱 | Radar 持续监控，快速调整定位 |
| GPU 资源不足做训练实验 | 无法证明效果 | 先用 7B 小模型验证，后续再扩大 |

---

## 八、待确认事项

以下事项需要在 Phase 0 阶段确认：

1. **项目名**：`agent-trajectory-hub` 还是其他？
2. **首批任务来源**：SWE-bench Pro 公开集（731 个中选 100 个）？还是 SWE-bench Verified？
3. **首批 Agent 框架**：OpenHands？SWE-agent？还是两者都用？
4. **训练资源**：有 GPU 资源做微调实验吗？需要多大规模？
5. **标注资源**：Phase 1 的 30 对人工校准由谁来做？
